{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c50a5e5-a2d9-4475-8198-4546664ac6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from models.models import get_model\n",
    "from utils.utils import load_model\n",
    "from mmcv.cnn.utils.flops_counter import add_flops_counting_methods, flops_to_string, params_to_string\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b18c479-47e7-4ae7-9ff6-f77d6dd0ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--test_batch_size', default=4, type=int)\n",
    "    parser.add_argument('--num_classes', default=1, type=int)\n",
    "    parser.add_argument('--epochs', default=150, type=int)\n",
    "    parser.add_argument('--perturb_input', default=None, type=float)\n",
    "    parser.add_argument('--perturb_model', default=None, type=float)\n",
    "    parser.add_argument('--no_GT', action='store_false',\n",
    "                        help=\"Ground Truth not available\")\n",
    "\n",
    "    # Input parameters\n",
    "    parser.add_argument('--input', nargs='+', type=int)\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model_name', type=str, default = 'unet',\n",
    "                        help=\"Provide Model.\")\n",
    "    parser.add_argument('--load_from', type=str, default=None,\n",
    "                        help=\"Path to the model weights.\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet101', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=384, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_frames', default=36, type=int,\n",
    "                        help=\"Number of frames\")\n",
    "    parser.add_argument('--num_ins', default=1, type=int,\n",
    "                        help=\"Number of instances\")\n",
    "    parser.add_argument('--num_queries', default=36, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_false',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--no_labels_loss', dest='labels_loss', action='store_false',\n",
    "                        help=\"Enables labels losses\")\n",
    "    parser.add_argument('--no_boxes_loss', dest='boxes_loss', action='store_false',\n",
    "                        help=\"Enables bounding box losses\")\n",
    "    parser.add_argument('--no_L1_loss', dest='L1_loss', action='store_false',\n",
    "                        help=\"Enables L1 losses for bboxes\")\n",
    "    parser.add_argument('--no_giou_loss', dest='giou_loss', action='store_false',\n",
    "                        help=\"Enables Generalized IOU losses for bboxes\")\n",
    "    parser.add_argument('--no_focal_loss', dest='focal_loss', action='store_false',\n",
    "                        help=\"Enables Focal losses for mask\")\n",
    "    parser.add_argument('--no_dice_loss', dest='dice_loss', action='store_false',\n",
    "                        help=\"Enables dice losses for mask\")\n",
    "    \n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--wrist', action='store_true',\n",
    "                        help=\"If true, model will train for only first 40 frames. i.e. wrist area\")\n",
    "    parser.add_argument('--save_clip', action='store_true',\n",
    "                        help=\"If true, model will save the clip with prediction contours.\")\n",
    "    parser.add_argument('--data_path', default='data/test/')\n",
    "    parser.add_argument('--save_path', default='results.json')\n",
    "    parser.add_argument('--dataset_file', default='ytvos')\n",
    "    parser.add_argument('--coco_path', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output_ytvos',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda:1',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    #parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--eval', action='store_false')\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977d8bfd-d154-489e-ab08-e5d591ec401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('Model training and evaluation script',\n",
    "                             parents=[get_args_parser()])\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb709ef-5ae3-4159-b30b-44b799deea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karan-mig/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /home/karan-mig/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|████████████████████████████████████████| 171M/171M [00:37<00:00, 4.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet101\n",
      "Average flops 26.15 GFLOPs\n",
      "Parameters 75.69 M\n",
      "Frames per second 119.78132876877679\n"
     ]
    }
   ],
   "source": [
    "# args.backbone = 'resnet50'\n",
    "args.model_name = 'vistr'\n",
    "\n",
    "if args.model_name != 'vistr':\n",
    "    model = get_model(args)\n",
    "else:\n",
    "    args.masks = True\n",
    "    model, _, _ = get_model(args)\n",
    "\n",
    "model.to(args.device)\n",
    "model.eval()\n",
    "model = add_flops_counting_methods(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    model.start_flops_count()\n",
    "    if args.model_name == 'vistr':\n",
    "        a = torch.rand((36,3,448,336))\n",
    "    \n",
    "    elif args.model_name == 'siam_unet':\n",
    "        a = torch.rand((16,1,448,336))\n",
    "        b = torch.rand((16,1,144,80))\n",
    "    \n",
    "    elif args.model_name == 'lstm_unet':\n",
    "        a = torch.rand((16,10,1,448,336))\n",
    "    \n",
    "    else:\n",
    "        a = torch.rand((16,1,448,336))\n",
    "        \n",
    "    count_time = []\n",
    "    if args.model_name == 'siam_unet':\n",
    "        for _ in range(100):\n",
    "            \n",
    "            start = time.time()\n",
    "            output = model(a.to(args.device), b.to(args.device))\n",
    "            end = time.time()\n",
    "            count_time.append(end-start)\n",
    "\n",
    "    else:\n",
    "        for _ in range(100):\n",
    "            \n",
    "            start = time.time()\n",
    "            output = model(a.to(args.device))\n",
    "            end = time.time()\n",
    "            count_time.append(end-start)\n",
    "    \n",
    "    AVG_flops, params_count = model.compute_average_flops_cost()\n",
    "    print('Average flops',flops_to_string(AVG_flops))\n",
    "    print('Parameters',params_to_string(params_count))\n",
    "    \n",
    "    if args.model_name == 'vistr':\n",
    "        print('Frames per second', 36/np.mean(count_time))\n",
    "    else:\n",
    "        print('Frames per second', 16/np.mean(count_time))\n",
    "        \n",
    "    model.stop_flops_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
