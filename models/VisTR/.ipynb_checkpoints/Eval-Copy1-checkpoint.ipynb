{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30b4d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karan/anaconda3/envs/tr/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pycocotools.mask as mask_util\n",
    "import re\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import pycocotools.mask as mask_utils\n",
    "from mmcv.cnn.utils.flops_counter import add_flops_counting_methods, flops_to_string, params_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4666bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise(object):\n",
    "    def __init__(self, eta = 0.5):\n",
    "        self.eta = eta\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + self.eta*tensor\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(eta={0.5}, std={1})'.format(self.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad97aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746ae8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=150, type=int)\n",
    "    parser.add_argument('--lr_drop', default=100, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model_path', type=str, default=None,\n",
    "                        help=\"Path to the model weights.\")\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet101', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=384, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_frames', default=36, type=int,\n",
    "                        help=\"Number of frames\")\n",
    "    parser.add_argument('--num_ins', default=1, type=int,\n",
    "                        help=\"Number of instances\")\n",
    "    parser.add_argument('--num_queries', default=36, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "    parser.add_argument('--no_labels_loss', dest='labels_loss', action='store_false',\n",
    "                        help=\"Enables labels losses\")\n",
    "    parser.add_argument('--no_boxes_loss', dest='boxes_loss', action='store_false',\n",
    "                        help=\"Enables bounding box losses\")\n",
    "    parser.add_argument('--no_L1_loss', dest='L1_loss', action='store_false',\n",
    "                        help=\"Enables L1 losses for bboxes\")\n",
    "    parser.add_argument('--no_giou_loss', dest='giou_loss', action='store_false',\n",
    "                        help=\"Enables Generalized IOU losses for bboxes\")\n",
    "    parser.add_argument('--no_focal_loss', dest='focal_loss', action='store_false',\n",
    "                        help=\"Enables Focal losses for mask\")\n",
    "    parser.add_argument('--no_dice_loss', dest='dice_loss', action='store_false',\n",
    "                        help=\"Enables dice losses for mask\")\n",
    "    \n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--img_path', default='data/ytvos/valid/JPEGImages/')\n",
    "    parser.add_argument('--ann_path', default='data/ytvos/annotations/instances_val_sub.json')\n",
    "    parser.add_argument('--save_path', default='results.json')\n",
    "    parser.add_argument('--dataset_file', default='ytvos')\n",
    "    parser.add_argument('--coco_path', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='output_ytvos',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    #parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--eval', action='store_false')\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd60b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b.cpu() * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def get_bbox(mask_list):\n",
    "    return torch.cat([masks_to_boxes(mask) for mask in mask_list], dim=0)\n",
    "\n",
    "pat=re.compile(\"(\\d+)\\D*$\")\n",
    "\n",
    "def key_func(x):\n",
    "    mat=pat.search(os.path.split(x)[-1]) # match last group of digits\n",
    "    if mat is None:\n",
    "        return x\n",
    "    return \"{:>10}\".format(mat.group(1)) # right align to 10 digits\n",
    "\n",
    "def get_metric(pred, truth):\n",
    "    # Sensitivity == Recall\n",
    "    SE = PC = F1 = DC = 0\n",
    "    \n",
    "    # SR : Segmentation Result\n",
    "    # GT : Ground Truth\n",
    "    SR, GT = pred, truth\n",
    "\n",
    "    # TP : True Positive\n",
    "    TP = ((SR==1)&(GT==1)).sum().item()\n",
    "\n",
    "    # FN : False Negative\n",
    "    FN = ((SR==0)&(GT==1)).sum().item()\n",
    "\n",
    "    # FP : False Positive\n",
    "    FP = ((SR==1)&(GT==0)).sum().item()\n",
    "\n",
    "    Inter = TP\n",
    "    Union = SR.sum().item() + GT.sum().item()\n",
    "    SE = float(TP)/(float(TP+FN) + 1e-6) #Recall\n",
    "    PC = float(TP)/(float(TP+FP) + 1e-6) #Precision\n",
    "    F1 = 2*SE*PC/(SE+PC + 1e-6) #F1 Score\n",
    "    DC = float(2*Inter)/(float(Union) + 1e-6) #Dice Score\n",
    "\n",
    "    return np.array([SE, PC, F1, DC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac39139",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('VisTR Inference script',\n",
    "                                 parents=[get_args_parser()])\n",
    "args = parser.parse_args(\"\")\n",
    "args\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "args.num_classes = 41\n",
    "args.masks = True\n",
    "args.model_path = 'r101_vistr/checkpoint0003.pth'\n",
    "args.device = 'cuda:0'\n",
    "\n",
    "#change these to 1 & 36\n",
    "args.num_ins = 1\n",
    "args.num_queries = 36\n",
    "args.num_frames = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25774f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_dir = sorted(glob.glob('../Dissertation/aster_updated_data_22_01_2022/Test/*'))\n",
    "test_file_dir\n",
    "\n",
    "test_img_path = []\n",
    "test_msk_path = []\n",
    "\n",
    "num_frames = args.num_frames\n",
    "num_ins = args.num_ins\n",
    "\n",
    "for path in test_file_dir:\n",
    "    test_img_path.append(sorted(glob.glob(path+'/*_0001_IMAGES/images/*.jpg'), key=key_func))\n",
    "    test_msk_path.append(sorted(glob.glob(path+'/*_0001_IMAGES/masks/*.png'), key=key_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90547630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karan/anaconda3/envs/tr/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/karan/anaconda3/envs/tr/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 75709217\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model, criterion, postprocessors = build_model(args)\n",
    "\n",
    "    \n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3677ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, image):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0700af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_img_path, num, model, n_frames = args.num_frames):\n",
    "    \n",
    "#     model = add_flops_counting_methods(model)\n",
    "    vid_length = len(test_img_path[num])\n",
    "#     vid_length = 40\n",
    "    vid = test_img_path[num]\n",
    "\n",
    "    pred_masks_seq_2 = []\n",
    "    pred_score_2 = []\n",
    "\n",
    "    im = Image.open(test_img_path[0][0])\n",
    "\n",
    "#     for i in tqdm(range(0, 40 ,n_frames), desc = 'Testing', leave = False):\n",
    "    for i in tqdm(range(0, vid_length ,n_frames), desc = 'Testing', leave = False):\n",
    "        start = i\n",
    "        end = min(i+n_frames,vid_length)\n",
    "        image = [transform(Image.open(frame)) for frame in vid[start:end]]\n",
    "        image = [img.repeat(3,1,1).unsqueeze(0).to(device) for img in image]\n",
    "        image = torch.cat(image,dim=0)\n",
    "        clip_len = end - start\n",
    "        if clip_len < n_frames:\n",
    "            image = torch.cat([image for _ in range(math.ceil(n_frames/clip_len))],dim=0)\n",
    "            image = image[:n_frames]\n",
    "#         model.start_flops_count()\n",
    "#         start = time.time()\n",
    "        outputs = get_pred(model, image)\n",
    "#         end = time.time()\n",
    "#         tim.append(len(imgs)/(end-start))\n",
    "#         AVG_flops, params_count = model.compute_average_flops_cost()\n",
    "#         print('Average flops',flops_to_string(AVG_flops))\n",
    "#         print('Parameters',params_to_string(params_count))\n",
    "#         model.stop_flops_count()\n",
    "        # end of model inference\n",
    "        logits, boxes, masks = outputs['pred_logits'].softmax(-1)[0,:,:-1], outputs['pred_boxes'][0], outputs['pred_masks'][0]\n",
    "        pred_masks = F.interpolate(masks.reshape(n_frames,num_ins,masks.shape[-2],masks.shape[-1]),(im.size[1],im.size[0]),mode=\"bilinear\").sigmoid().cpu().detach().numpy()>0.5\n",
    "        pred_logits = logits.reshape(n_frames,num_ins,logits.shape[-1]).cpu().detach().numpy()\n",
    "        pred_masks = pred_masks[:clip_len]\n",
    "        pred_logits = pred_logits[:clip_len]\n",
    "        pred_scores = np.max(pred_logits,axis=-1)\n",
    "        pred_logits = np.argmax(pred_logits,axis=-1)\n",
    "        temp = []\n",
    "        for m in range(num_ins):\n",
    "            if pred_masks[:,m].max()==0:\n",
    "                continue\n",
    "            score = pred_scores[:,m].mean()\n",
    "            #category_id = pred_logits[:,m][pred_scores[:,m].argmax()]\n",
    "            category_id = np.argmax(np.bincount(pred_logits[:,m]))\n",
    "            instance = {'score':float(score), 'category_id':int(category_id)}\n",
    "            temp.append(instance)\n",
    "        pred_score_2.append(temp)\n",
    "        pred_masks_seq_2.append(pred_masks)\n",
    "    pred_final_2 = [img[0] for batch in pred_masks_seq_2 for img in batch]\n",
    "    return pred_final_2, vid_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8752dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating mean and std of weights\n",
    "# state_dict = torch.load(args.model_path,  map_location='cpu')['model']\n",
    "# m2 =[]\n",
    "# m=[]\n",
    "# for key in state_dict.keys():\n",
    "#     m.append(torch.mean(state_dict[key]))\n",
    "#     m2.append(torch.mean(state_dict[key]**2))\n",
    "# ex = np.mean(m)\n",
    "# ex\n",
    "# ex2 = np.mean(m2)\n",
    "# ex2\n",
    "# print('mean:',ex,' std:',np.sqrt(ex2 - ex**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a111190b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for std:0.0303 >[0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for std:0.0606 >[0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for std:0.0909 >[0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a9db9049db4abe837b4e9b354598c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01d4e56b1624955aab31e146f5d30ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m pred_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_img_path)), leave \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     start = time.time()\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     output, vid_length \u001b[38;5;241m=\u001b[39m test_model(test_img_path, num, model, num_frames)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     end = time.time()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     print(vid_length/(end-start))\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     pred_mask\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(test_img_path, num, model, n_frames)\u001b[0m\n\u001b[1;32m     23\u001b[0m             image \u001b[38;5;241m=\u001b[39m image[:n_frames]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         model.start_flops_count()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         start = time.time()\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m get_pred(model, image)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         end = time.time()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         tim.append(len(imgs)/(end-start))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         AVG_flops, params_count = model.compute_average_flops_cost()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#         model.stop_flops_count()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# end of model inference\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         logits, boxes, masks \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_logits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m,:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_masks\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mget_pred\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(image)\n",
      "File \u001b[0;32m~/anaconda3/envs/tr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/VisTR-master/models/segmentation.py:86\u001b[0m, in \u001b[0;36mVisTRsegm.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, NestedTensor):\n\u001b[1;32m     85\u001b[0m     samples \u001b[38;5;241m=\u001b[39m nested_tensor_from_tensor_list(samples)\n\u001b[0;32m---> 86\u001b[0m features, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvistr\u001b[38;5;241m.\u001b[39mbackbone(samples)\n\u001b[1;32m     87\u001b[0m bs \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m src, mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecompose()\n",
      "File \u001b[0;32m~/anaconda3/envs/tr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/VisTR-master/models/backbone.py:107\u001b[0m, in \u001b[0;36mJoiner.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    105\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# position encoding\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     pos\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m1\u001b[39m](x)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, pos\n",
      "File \u001b[0;32m~/anaconda3/envs/tr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/VisTR-master/models/position_encoding.py:46\u001b[0m, in \u001b[0;36mPositionEmbeddingSine.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m     43\u001b[0m     x_embed \u001b[38;5;241m=\u001b[39m x_embed \u001b[38;5;241m/\u001b[39m (x_embed[:, :, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m eps) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m     45\u001b[0m dim_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos_feats, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 46\u001b[0m dim_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (dim_t \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos_feats)\n\u001b[1;32m     48\u001b[0m pos_x \u001b[38;5;241m=\u001b[39m x_embed[:, :, :, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m/\u001b[39m dim_t\n\u001b[1;32m     49\u001b[0m pos_y \u001b[38;5;241m=\u001b[39m y_embed[:, :, :, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m/\u001b[39m dim_t\n",
      "File \u001b[0;32m~/anaconda3/envs/tr/lib/python3.11/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tr/lib/python3.11/site-packages/torch/_tensor.py:879\u001b[0m, in \u001b[0;36mTensor.__rpow__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rpow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    878\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mresult_type(other, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(other, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stds = [-0.1, -0.08, -0.06, -0.04, -0.02, 0, 0.02, 0.04, 0.06, 0.08, 0.1]#, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "res_on_std = []\n",
    "\n",
    "for std in stds:\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.2316], [0.2038]), #mean #standard deviation\n",
    "#             AddGaussianNoise(mean=0., std=std)\n",
    "    ])\n",
    "\n",
    "    noice = AddNoise(eta=std)\n",
    "    state_dict = torch.load(args.model_path,  map_location='cpu')['model']\n",
    "    if noice:\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = noice(state_dict[key])\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    test_res = []\n",
    "    pred_mask = []\n",
    "    for num in tqdm(range(len(test_img_path)), leave = False):\n",
    "    #     start = time.time()\n",
    "        output, vid_length = test_model(test_img_path, num, model, num_frames)\n",
    "    #     end = time.time()\n",
    "    #     print(vid_length/(end-start))\n",
    "        pred_mask.append(output)\n",
    "    #     print(len(output))\n",
    "        res_2 = []\n",
    "\n",
    "        for j in range(vid_length):\n",
    "            GT = np.array(Image.open(test_msk_path[num][j]))/255\n",
    "            SR = output[j]\n",
    "\n",
    "            res_2.append(get_metric(SR, GT))\n",
    "        res_2 = np.array(res_2)\n",
    "#         print(f'{num+1}:',res_2.mean(axis=0))\n",
    "        test_res.append(res_2)\n",
    "    mean_metric = np.mean([i.mean(axis=0) for i in test_res], axis=0)\n",
    "    res_on_std.append(mean_metric)\n",
    "    print(f'for std:{std} >{mean_metric}') #chckpnt 7 with first 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_run = np.array(multiple_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_run.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8996022",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.round(np.mean(multiple_run, axis=0),3).T\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stds)):\n",
    "    print(f'{stds[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = np.round(np.std(multiple_run, axis=0),4)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for std:0 >[0.83690573 0.85833555 0.83802441 0.83802489]\n",
    "for std:0.1 >[0.82723017 0.86671863 0.83654147 0.83654195]\n",
    "for std:0.2 >[0.81205674 0.86474631 0.82599101 0.82599149]\n",
    "for std:0.3 >[0.8026464  0.85994068 0.81760362 0.8176041 ]\n",
    "for std:0.4 >[0.78327907 0.84819331 0.80051047 0.80051094]\n",
    "for std:0.5 >[0.77034189 0.84139687 0.7871163  0.78711677]\n",
    "for std:0.6 >[0.74281318 0.82671354 0.76420754 0.764208  ]\n",
    "for std:0.7 >[0.7144488  0.81369497 0.74036554 0.74036598]\n",
    "for std:0.8 >[0.69103701 0.80726473 0.72417577 0.72417621]\n",
    "for std:0.9 >[0.65231087 0.79418556 0.69225012 0.69225054]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c330f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 7 with first 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "std - 0 0.83690573, 0.85833555, 0.83802441, 0.83802489 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 7 with first 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b076fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in range(len(test_file_dir)):\n",
    "    name = test_file_dir[i].split('/')[-1]\n",
    "    \n",
    "    masks = np.array(pred_mask[i]).transpose((1,2,0))\n",
    "    masks = np.array(masks, order='F', dtype='uint8')\n",
    "    masks = mask_utils.encode(masks)\n",
    "    for msk in masks:\n",
    "        msk['counts'] = msk['counts'].decode()\n",
    "    res[name] = masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7520ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(res, open('pred_results_test.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff020e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ca617",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i.mean(axis=0) for i in test_res], axis=0) #chckpnt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_video(img, msk, type_ = None):\n",
    "    if not isinstance(img, np.ndarray): img = np.array(img)\n",
    "    if not isinstance(msk, np.ndarray): msk = np.array(msk)\n",
    "    if img.dtype!='uint8': img= (img*255).astype(np.uint8)\n",
    "    if msk.dtype!='uint8': msk= (msk*255).astype(np.uint8)\n",
    "    if(len(img.shape)<3): img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    cnts = cv2.findContours(msk, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    if type_ == 'GT':\n",
    "        for c in cnts:\n",
    "            cv2.drawContours(img, [c], -1, (0, 0, 255), thickness=2)\n",
    "    if type_ == 'Pred':\n",
    "        for c in cnts:\n",
    "            cv2.drawContours(img, [c], -1, (0, 255, 0), thickness=2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0de722",
   "metadata": {},
   "outputs": [],
   "source": [
    "frameSize = (336, 448) # width x height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_VisTR_5.avi', \n",
    "                      fourcc, 15,\n",
    "                      frameSize,)\n",
    "#                       isColor = False)\n",
    "\n",
    "for i in range(len(test_img_path[5])):\n",
    "    \n",
    "    img = Image.open(test_img_path[5][i]) #frame\n",
    "    msk = Image.open(test_msk_path[5][i]) #ground Truth\n",
    "    temp = create_pred_video(img, msk, 'GT')\n",
    "    out_frame = create_pred_video(temp,pred_mask[5][i], 'Pred')\n",
    "    out.write(out_frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dir = sorted(glob.glob('../Dissertation/aster_updated_data_22_01_2022/NORMAL WRIST MEDIAN 1.5CM/*'))\n",
    "# file_dir = sorted(glob.glob('../Dissertation/aster_updated_data_22_01_2022/SEVERE CTS WRIST 1.5CM/*'))\n",
    "# file_dir = sorted(glob.glob('../Dissertation/aster_updated_data_22_01_2022/CTS 1.5CM AND 3CM/*'))\n",
    "file_dir = sorted(glob.glob('../Dissertation/aster_updated_data_22_01_2022/full/*'))\n",
    "\n",
    "img_path = []\n",
    "msk_path = []\n",
    "\n",
    "for path in file_dir:\n",
    "    img_path.append(sorted(glob.glob(path+'/*.jpg'), key=key_func))\n",
    "#     img_path.append(sorted(glob.glob(path+'/frames/*.jpg'), key=key_func))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba1f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_res = []\n",
    "pred_mask = []\n",
    "for num in tqdm(range(len(img_path))):\n",
    "    start = time.time()\n",
    "    output, vid_length = test_model(img_path, num, model, num_frames)\n",
    "    end = time.time()\n",
    "#     print(vid_length/(end-start))\n",
    "#     break\n",
    "    pred_mask.append(output)\n",
    "    print(vid_length/(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b03df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in range(len(file_dir)):\n",
    "    name = file_dir[i].split('/')[-1]\n",
    "    \n",
    "    masks = np.array(pred_mask[i]).transpose((1,2,0))\n",
    "    masks = np.array(masks, order='F', dtype='uint8')\n",
    "    masks = mask_utils.encode(masks)\n",
    "    for msk in masks:\n",
    "        msk['counts'] = msk['counts'].decode()\n",
    "    res[name] = masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(res, open('pred_results_NWM.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_num = 1\n",
    "name = file_dir[vid_num].split('/')[-1]\n",
    "frameSize = (336, 448) # width x height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(str(vid_num)+'_'+name+'.avi', \n",
    "                      fourcc, 15,\n",
    "                      frameSize,)\n",
    "#                       isColor = False)\n",
    "\n",
    "for i in range(len(img_path[vid_num])):\n",
    "    \n",
    "    img = Image.open(img_path[vid_num][i]) #frame\n",
    "    out_frame = create_pred_video(img,pred_mask[vid_num][i], 'Pred')\n",
    "    out.write(out_frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388315b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Image.open(test_img_path[num][0])\n",
    "y = Image.open(test_msk_path[num][0])\n",
    "# create_pred_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob('../Dissertation/aster_updated_data_22_01_2022/full/*'))\n",
    "test_full_vid = []\n",
    "for path in paths:\n",
    "    test_full_vid.append(sorted(glob.glob(path + '/*.jpg'), key=key_func))\n",
    "test_full_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0396b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image shape into 448,336\n",
    "for img_path in tqdm(test_full_vid[1], total=len(test_full_vid[1])):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img.shape != (448,336):\n",
    "        cv2.imwrite(img_path, img[4:452,3:339])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mask = []\n",
    "start = time.time()\n",
    "for num in tqdm(range(len(test_full_vid))):\n",
    "    output = test_model(test_full_vid, num, model, n_frames = 1)\n",
    "    p_mask.append(output)\n",
    "    \n",
    "end = time.time()\n",
    "print('time taken:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_full_vid[0])/(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9480ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "frameSize = (336, 448) # width x height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_VisTR_144344_full.avi', \n",
    "                      fourcc, 15,\n",
    "                      frameSize,)\n",
    "#                       isColor = False)\n",
    "\n",
    "for i in range(len(test_full_vid[0])):\n",
    "    \n",
    "    img = Image.open(test_full_vid[0][i]) #frame\n",
    "#     msk = Image.open(test_msk_path[5][i]) #ground Truth\n",
    "#     temp = create_pred_video(img, msk, 'GT')\n",
    "    out_frame = create_pred_video(img,p_mask[0][i], 'Pred')\n",
    "    out.write(out_frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_full_vid[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
