{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6fa4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from TimeDistributed import TimeDistributed\n",
    "from ConvLSTM import ConvLSTM\n",
    "from unet_parts import *\n",
    "\n",
    "class LSTM_UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(LSTM_UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = TimeDistributed(DoubleConv(1, 64))\n",
    "        self.down1 = TimeDistributed(Down(64, 128))\n",
    "        self.down2 = TimeDistributed(Down(128, 256))\n",
    "        self.down3 = TimeDistributed(Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = TimeDistributed(Down(512, 512))\n",
    "        \n",
    "        self.lstm = ConvLSTM(input_dim = 512,\n",
    "                    hidden_dim=512,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "        \n",
    "        self.comp1 = nn.Conv2d(64, 62, kernel_size=1) #2\n",
    "        self.comp2= nn.Conv2d(128, 120, kernel_size=1) #8\n",
    "        self.comp3= nn.Conv2d(256, 224, kernel_size=1) #32\n",
    "        self.comp4= nn.Conv2d(512, 384, kernel_size=1) #128\n",
    "        \n",
    "        self.lstm_up4 = nn.ConvTranspose2d(512, 128, kernel_size=2, stride=2)\n",
    "        self.lstm_up3 = nn.ConvTranspose2d(512, 32, kernel_size=4, stride=4)\n",
    "        self.lstm_up2 = nn.ConvTranspose2d(512, 8, kernel_size=8, stride=8)\n",
    "        self.lstm_up1 = nn.ConvTranspose2d(512, 2, kernel_size=16, stride=16)\n",
    "\n",
    "        self.up1 = (Up(1024, 512, bilinear))\n",
    "        self.up2 = (Up(512, 256, bilinear))\n",
    "        self.up3 = (Up(256, 128, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        _ , [(h, _)] = self.lstm(x5)\n",
    "        \n",
    "        l1 = self.lstm_up1(h)\n",
    "        l2 = self.lstm_up2(h)\n",
    "        l3 = self.lstm_up3(h)\n",
    "        l4 = self.lstm_up4(h)\n",
    "        \n",
    "        c1 = self.comp1(x1[:,-1,...])\n",
    "        c2 = self.comp2(x2[:,-1,...])\n",
    "        c3 = self.comp3(x3[:,-1,...])\n",
    "        c4 = self.comp4(x4[:,-1,...])\n",
    "\n",
    "        x = self.up1(torch.cat([x5[:,-1,...],h], dim=1), torch.cat([l4,c4], dim=1))\n",
    "        x = self.up2(x, torch.cat([l3,c3], dim=1))\n",
    "        x = self.up3(x, torch.cat([l2,c2], dim=1))\n",
    "        x = self.up4(x, torch.cat([l1,c1], dim=1))\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eba8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from LSTM_v1 import LSTM_UNet\n",
    "model = LSTM_UNet(n_channels=1, n_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2c22bf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = torch.rand([1,10,1,448,336])\n",
    "\n",
    "out = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64c836ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.672897"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32cd4365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 448, 336])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5650fa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4a70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57744a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "    \n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "from TimeDistributed import TimeDistributed\n",
    "from ConvLSTM import ConvLSTM\n",
    "from unet_parts import *\n",
    "\n",
    "class Att_LSTM_UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, num_filter = 32, bilinear=False):\n",
    "        super(Att_LSTM_UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = TimeDistributed(DoubleConv(n_channels, num_filter))\n",
    "        self.down1 = TimeDistributed(Down(num_filter, num_filter*2))\n",
    "        self.down2 = TimeDistributed(Down(num_filter*2, num_filter*4))\n",
    "        self.down3 = TimeDistributed(Down(num_filter*4, num_filter*8))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = TimeDistributed(Down(num_filter*8, num_filter*16 // factor))\n",
    "        \n",
    "        self.lstm = ConvLSTM(input_dim = num_filter*16 // factor,\n",
    "                    hidden_dim=num_filter*16 // factor,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "        \n",
    "        self.Up5 = up_conv(ch_in=num_filter*16,ch_out=num_filter*8)\n",
    "        self.Att5 = Attention_block(F_g=num_filter*8,F_l=num_filter*8,F_int=num_filter*4)\n",
    "        self.Up_conv5 = DoubleConv(in_channels=num_filter*16, out_channels=num_filter*8)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=num_filter*8,ch_out=num_filter*4)\n",
    "        self.Att4 = Attention_block(F_g=num_filter*4,F_l=num_filter*4,F_int=num_filter*2)\n",
    "        self.Up_conv4 = DoubleConv(in_channels=num_filter*8, out_channels=num_filter*4)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=num_filter*4,ch_out=num_filter*2)\n",
    "        self.Att3 = Attention_block(F_g=num_filter*2,F_l=num_filter*2,F_int=num_filter)\n",
    "        self.Up_conv3 = DoubleConv(in_channels=num_filter*4, out_channels=num_filter*2)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=num_filter*2,ch_out=num_filter)\n",
    "        self.Att2 = Attention_block(F_g=num_filter,F_l=num_filter,F_int=num_filter//2)\n",
    "        self.Up_conv2 = DoubleConv(in_channels=num_filter*2, out_channels=num_filter)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(num_filter,n_classes,kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        _ , [(h, c)] = self.lstm(x5)\n",
    "        print(x5.shape, h.shape)\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(c)\n",
    "        x4 = self.Att5(g=d5,x=x4[:,-1,...])\n",
    "        d5 = torch.cat((x4,d5),dim=1)        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3[:,-1,...])\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2[:,-1,...])\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1[:,-1,...])\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "        \n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0800e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand([4,10,512,8,8])\n",
    "num_filter=32\n",
    "factor = 1\n",
    "layer = ConvLSTM(input_dim = num_filter*16 // factor,\n",
    "                    hidden_dim=num_filter*16 // factor,\n",
    "                    kernel_size=(3,3),\n",
    "                    num_layers=2,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                    return_all_layers=False)\n",
    "[x], _ = layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb8875fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Att_LSTM_UNet(n_channels=1, n_classes=1, num_filter = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3d25523",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512, 28, 21]) torch.Size([1, 512, 28, 21])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([1,10,1,448,336])\n",
    "\n",
    "out = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab292d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.475389"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17852759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 448, 336])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690923d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 448, 336])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "    \n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40eb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =torch.rand([4,10,1,448,336])\n",
    "model = Att_LSTM_UNet(n_channels=1, n_classes=1, num_filter = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901048d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_load = './checkpoints_LSTM/checkpoint_epoch50.pth'\n",
    "\n",
    "if args_load:\n",
    "    state_dict = torch.load(args_load)#, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0598e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_children():\n",
    "    if not name.startswith('params'):\n",
    "        print(name)\n",
    "        print(module)\n",
    "        print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943edcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
